{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa494fd-0c11-473c-904a-10ad65ccd993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All imports\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import mplfinance as mpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec9349-2642-488e-8147-f1aa02a40a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_candle(symbol, interval, period):\n",
    "    ticker = yf.Ticker(symbol).history(interval = interval, period = period, prepost = True)\n",
    "    mpf.plot(ticker, type = 'candle', style = 'charles', ylabel = 'Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39811416-7572-4944-b33b-7f43198645ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$SPX: possibly delisted; No price data found  (period=1mo)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$SPX: possibly delisted; No price data found  (period=1mo)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expect data.index as DatetimeIndex",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_candle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSPX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5m\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m, in \u001b[0;36mplot_candle\u001b[0;34m(symbol, interval, period)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_candle\u001b[39m(symbol, interval, period):\n\u001b[1;32m      2\u001b[0m     ticker \u001b[38;5;241m=\u001b[39m yf\u001b[38;5;241m.\u001b[39mTicker(symbol)\u001b[38;5;241m.\u001b[39mhistory(interval \u001b[38;5;241m=\u001b[39m interval, period \u001b[38;5;241m=\u001b[39m period, prepost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m     mpf\u001b[38;5;241m.\u001b[39mplot(ticker, \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcandle\u001b[39m\u001b[38;5;124m'\u001b[39m, style \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharles\u001b[39m\u001b[38;5;124m'\u001b[39m, ylabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mplfinance/plotting.py:417\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(data, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# translate alias types:\u001b[39;00m\n\u001b[1;32m    415\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_valid_plot_types(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 417\u001b[0m dates,opens,highs,lows,closes,volumes \u001b[38;5;241m=\u001b[39m _check_and_prepare_data(data, config)\n\u001b[1;32m    419\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxlim\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m _check_and_convert_xlim_configuration(data, config)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m VALID_PMOVE_TYPES \u001b[38;5;129;01mand\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddplot\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mplfinance/_arg_validators.py:32\u001b[0m, in \u001b[0;36m_check_and_prepare_data\u001b[0;34m(data, config)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpect data as DataFrame\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mindex,pd\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mindexes\u001b[38;5;241m.\u001b[39mdatetimes\u001b[38;5;241m.\u001b[39mDatetimeIndex):\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpect data.index as DatetimeIndex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# We will not be fully case-insensitive (since Pandas columns as NOT case-insensitive)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# but because so many people have requested it, for the default column names we will\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# try both Capitalized and lower case:\u001b[39;00m\n\u001b[1;32m     37\u001b[0m columns \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Expect data.index as DatetimeIndex"
     ]
    }
   ],
   "source": [
    "plot_candle('SPX', '5m', '1mo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ba0270-ef0f-45f7-a22a-1b0ea0d37982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# https://youtu.be/tepxdcepTbY\n",
    "@author: Sreenivas Bhattiprolu\n",
    "\n",
    "Code tested on Tensorflow: 2.2.0\n",
    "    Keras: 2.4.3\n",
    "\n",
    "dataset: https://finance.yahoo.com/quote/GE/history/\n",
    "Also try S&P: https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "#Read the csv file\n",
    "df = pd.read_csv('data/GE.csv')\n",
    "print(df.head()) #7 columns, including the Date. \n",
    "\n",
    "#Separate dates for future plotting\n",
    "train_dates = pd.to_datetime(df['Date'])\n",
    "print(train_dates.tail(15)) #Check last few dates. \n",
    "\n",
    "#Variables for training\n",
    "cols = list(df)[1:6]\n",
    "#Date and volume columns are not used in training. \n",
    "print(cols) #['Open', 'High', 'Low', 'Close', 'Adj Close']\n",
    "\n",
    "#New dataframe with only training data - 5 columns\n",
    "df_for_training = df[cols].astype(float)\n",
    "\n",
    "# df_for_plot=df_for_training.tail(5000)\n",
    "# df_for_plot.plot.line()\n",
    "\n",
    "#LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n",
    "# normalize the dataset\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(df_for_training)\n",
    "df_for_training_scaled = scaler.transform(df_for_training)\n",
    "\n",
    "\n",
    "#As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features. \n",
    "#In this example, the n_features is 5. We will make timesteps = 14 (past days data used for training). \n",
    "\n",
    "#Empty lists to be populated using formatted training data\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "n_future = 1   # Number of days we want to look into the future based on the past days.\n",
    "n_past = 14  # Number of past days we want to use to predict the future.\n",
    "\n",
    "#Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
    "#In my example, my df_for_training_scaled has a shape (12823, 5)\n",
    "#12823 refers to the number of data points and 5 refers to the columns (multi-variables).\n",
    "for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
    "    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "\n",
    "print('trainX shape == {}.'.format(trainX.shape))\n",
    "print('trainY shape == {}.'.format(trainY.shape))\n",
    "\n",
    "#In my case, trainX has a shape (12809, 14, 5). \n",
    "#12809 because we are looking back 14 days (12823 - 14 = 12809). \n",
    "#Remember that we cannot look back 14 days until we get to the 15th day. \n",
    "#Also, trainY has a shape (12809, 1). Our model only predicts a single value, but \n",
    "#it needs multiple variables (5 in my example) to make this prediction. \n",
    "#This is why we can only predict a single day after our training, the day after where our data ends.\n",
    "#To predict more days in future, we need all the 5 variables which we do not have. \n",
    "#We need to predict all variables if we want to do that. \n",
    "\n",
    "# define the Autoencoder model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(trainY.shape[1]))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(trainX, trainY, epochs=5, batch_size=16, validation_split=0.1, verbose=1)\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "#Predicting...\n",
    "#Libraries that will help us extract only business days in the US.\n",
    "#Otherwise our dates would be wrong when we look back (or forward).  \n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
    "#Remember that we can only predict one day in future as our model needs 5 variables\n",
    "#as inputs for prediction. We only have all 5 variables until the last day in our dataset.\n",
    "n_past = 16\n",
    "n_days_for_prediction=15  #let us predict past 15 days\n",
    "\n",
    "predict_period_dates = pd.date_range(list(train_dates)[-n_past], periods=n_days_for_prediction, freq=us_bd).tolist()\n",
    "print(predict_period_dates)\n",
    "\n",
    "#Make prediction\n",
    "prediction = model.predict(trainX[-n_days_for_prediction:]) #shape = (n, 1) where n is the n_days_for_prediction\n",
    "\n",
    "#Perform inverse transformation to rescale back to original range\n",
    "#Since we used 5 variables for transform, the inverse expects same dimensions\n",
    "#Therefore, let us copy our values 5 times and discard them after inverse transform\n",
    "prediction_copies = np.repeat(prediction, df_for_training.shape[1], axis=-1)\n",
    "y_pred_future = scaler.inverse_transform(prediction_copies)[:,0]\n",
    "\n",
    "\n",
    "# Convert timestamp to date\n",
    "forecast_dates = []\n",
    "for time_i in predict_period_dates:\n",
    "    forecast_dates.append(time_i.date())\n",
    "    \n",
    "df_forecast = pd.DataFrame({'Date':np.array(forecast_dates), 'Open':y_pred_future})\n",
    "df_forecast['Date']=pd.to_datetime(df_forecast['Date'])\n",
    "\n",
    "\n",
    "original = df[['Date', 'Open']]\n",
    "original['Date']=pd.to_datetime(original['Date'])\n",
    "original = original.loc[original['Date'] >= '2020-5-1']\n",
    "\n",
    "sns.lineplot(original['Date'], original['Open'])\n",
    "sns.lineplot(df_forecast['Date'], df_forecast['Open'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213fb80-c2d2-4732-9fac-c086b86bb35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
